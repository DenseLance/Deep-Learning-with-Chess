{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Chess Using Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "from math import exp\n",
    "import numpy as np\n",
    "from random import random, choice\n",
    "\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy, GreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "import chess\n",
    "import chess.pgn\n",
    "import chess.svg\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category = UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining state and action spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chess_positions():\n",
    "    return [f\"{char}{num + 1}\" for num in range(8) for char in \"abcdefgh\"]\n",
    "\n",
    "def knight_moves():\n",
    "    positions = chess_positions()\n",
    "    moves = []\n",
    "    for position in positions:\n",
    "        position_alpha_ord = ord(position[0])\n",
    "        position_digit = int(position[1])\n",
    "        moves.append(f\"{position}{chr(position_alpha_ord + 2)}{min(max(position_digit + 1, 0), 9)}\")\n",
    "        moves.append(f\"{position}{chr(position_alpha_ord + 2)}{min(max(position_digit - 1, 0), 9)}\")\n",
    "        moves.append(f\"{position}{chr(position_alpha_ord - 2)}{min(max(position_digit + 1, 0), 9)}\")\n",
    "        moves.append(f\"{position}{chr(position_alpha_ord - 2)}{min(max(position_digit - 1, 0), 9)}\")\n",
    "        moves.append(f\"{position}{chr(position_alpha_ord + 1)}{min(max(position_digit + 2, 0), 9)}\")\n",
    "        moves.append(f\"{position}{chr(position_alpha_ord + 1)}{min(max(position_digit - 2, 0), 9)}\")\n",
    "        moves.append(f\"{position}{chr(position_alpha_ord - 1)}{min(max(position_digit + 2, 0), 9)}\")\n",
    "        moves.append(f\"{position}{chr(position_alpha_ord - 1)}{min(max(position_digit - 2, 0), 9)}\")\n",
    "\n",
    "    return [move for move in moves if move[-2:] in positions]\n",
    "\n",
    "def bishop_moves():\n",
    "    def helper(position, positions, x, y):\n",
    "        position_alpha_ord = ord(position[0])\n",
    "        position_digit = int(position[1])\n",
    "        next_pos = f\"{chr(position_alpha_ord + x)}{min(max(position_digit + y, 0), 9)}\"\n",
    "        if next_pos in positions:\n",
    "            return [next_pos] + helper(next_pos, positions, x, y)\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "    positions = chess_positions()\n",
    "    moves = []\n",
    "    for position in positions:\n",
    "        moves += [position + next_pos for next_pos in helper(position, positions, 1, 1)]\n",
    "        moves += [position + next_pos for next_pos in helper(position, positions, 1, -1)]\n",
    "        moves += [position + next_pos for next_pos in helper(position, positions, -1, 1)]\n",
    "        moves += [position + next_pos for next_pos in helper(position, positions, -1, -1)]\n",
    "    return moves\n",
    "\n",
    "def rook_moves():\n",
    "    def helper(position, positions, x, y):\n",
    "        position_alpha_ord = ord(position[0])\n",
    "        position_digit = int(position[1])\n",
    "        next_pos = f\"{chr(position_alpha_ord + x)}{min(max(position_digit + y, 0), 9)}\"\n",
    "        if next_pos in positions:\n",
    "            return [next_pos] + helper(next_pos, positions, x, y)\n",
    "        else:\n",
    "            return []\n",
    "    positions = chess_positions()\n",
    "    moves = []\n",
    "    for position in positions:\n",
    "        moves += [position + next_pos for next_pos in helper(position, positions, 1, 0)]\n",
    "        moves += [position + next_pos for next_pos in helper(position, positions, -1, 0)]\n",
    "        moves += [position + next_pos for next_pos in helper(position, positions, 0, 1)]\n",
    "        moves += [position + next_pos for next_pos in helper(position, positions, 0, -1)]\n",
    "    return moves\n",
    "\n",
    "def queen_moves():\n",
    "    return rook_moves() + bishop_moves()\n",
    "\n",
    "def pawn_promotion_moves():\n",
    "    def helper(position, positions, x, y):\n",
    "        position_alpha_ord = ord(position[0])\n",
    "        position_digit = int(position[1])\n",
    "        next_pos = f\"{chr(position_alpha_ord + x)}{min(max(position_digit + y, 0), 9)}\"\n",
    "        if next_pos in positions:\n",
    "            return [next_pos] + helper(next_pos, positions, x, y)\n",
    "        else:\n",
    "            return []\n",
    "    positions = chess_positions()\n",
    "    pawn_positions_white = [alpha + \"7\" for alpha in \"abcdefgh\"]\n",
    "    pawn_positions_black = [alpha + \"2\" for alpha in \"abcdefgh\"]\n",
    "    moves = []\n",
    "    for position in pawn_positions_white:\n",
    "        moves += [position + next_pos + \"q\" for next_pos in helper(position, positions, 0, 1)]\n",
    "        moves += [position + next_pos + \"r\" for next_pos in helper(position, positions, 0, 1)]\n",
    "        moves += [position + next_pos + \"b\" for next_pos in helper(position, positions, 0, 1)]\n",
    "        moves += [position + next_pos + \"n\" for next_pos in helper(position, positions, 0, 1)]\n",
    "        moves += [position + next_pos + \"q\" for next_pos in helper(position, positions, 1, 1)]\n",
    "        moves += [position + next_pos + \"r\" for next_pos in helper(position, positions, 1, 1)]\n",
    "        moves += [position + next_pos + \"b\" for next_pos in helper(position, positions, 1, 1)]\n",
    "        moves += [position + next_pos + \"n\" for next_pos in helper(position, positions, 1, 1)]\n",
    "        moves += [position + next_pos + \"q\" for next_pos in helper(position, positions, -1, 1)]\n",
    "        moves += [position + next_pos + \"r\" for next_pos in helper(position, positions, -1, 1)]\n",
    "        moves += [position + next_pos + \"b\" for next_pos in helper(position, positions, -1, 1)]\n",
    "        moves += [position + next_pos + \"n\" for next_pos in helper(position, positions, -1, 1)]\n",
    "    for position in pawn_positions_black:\n",
    "        moves += [position + next_pos + \"q\" for next_pos in helper(position, positions, 0, -1)]\n",
    "        moves += [position + next_pos + \"r\" for next_pos in helper(position, positions, 0, -1)]\n",
    "        moves += [position + next_pos + \"b\" for next_pos in helper(position, positions, 0, -1)]\n",
    "        moves += [position + next_pos + \"n\" for next_pos in helper(position, positions, 0, -1)]\n",
    "        moves += [position + next_pos + \"q\" for next_pos in helper(position, positions, 1, -1)]\n",
    "        moves += [position + next_pos + \"r\" for next_pos in helper(position, positions, 1, -1)]\n",
    "        moves += [position + next_pos + \"b\" for next_pos in helper(position, positions, 1, -1)]\n",
    "        moves += [position + next_pos + \"n\" for next_pos in helper(position, positions, 1, -1)]\n",
    "        moves += [position + next_pos + \"q\" for next_pos in helper(position, positions, -1, -1)]\n",
    "        moves += [position + next_pos + \"r\" for next_pos in helper(position, positions, -1, -1)]\n",
    "        moves += [position + next_pos + \"b\" for next_pos in helper(position, positions, -1, -1)]\n",
    "        moves += [position + next_pos + \"n\" for next_pos in helper(position, positions, -1, -1)]\n",
    "    \n",
    "    return moves\n",
    "\n",
    "def possible_moves():\n",
    "    # Naive way of determining action space, weeds out invalid actions\n",
    "    return knight_moves() + queen_moves() + pawn_promotion_moves()\n",
    "\n",
    "def valid_moves(board):\n",
    "    return tuple(move.uci() for move in board.legal_moves)\n",
    "\n",
    "def board_to_state(board):\n",
    "    \"\"\"\n",
    "    FEN (https://en.wikipedia.org/wiki/Forsyth%E2%80%93Edwards_Notation)\n",
    "    * White pieces are uppercase\n",
    "    * Black pieces are lowercase\n",
    "\n",
    "    Consists of 6 fields:\n",
    "        1. Piece placement (from white's perspective)\n",
    "        2. Active colour (w: white to move, b: black to move)\n",
    "        3. Castling availability (-: no castling, KQkq: both can castle, KQ: only white can castle,\n",
    "                                    kq: only black can castle, Q: only white can castle queenside,\n",
    "                                    k: only black can castle kingside)\n",
    "        4. En passant availability (-: no en passant, f6: en passant capture available on f6)\n",
    "        5. Half-move clock (used for 50 move rule, if reach 50 it is a draw)\n",
    "        6. Full-move clock (number of moves made by black since start of game)\n",
    "    \"\"\"\n",
    "    fields = board.fen().split(\" \")\n",
    "\n",
    "    # PIECE PLACEMENT\n",
    "    # Preferable to use one-hot encoding, did not use it because of high space complexity\n",
    "    # Categorical encoding is used instead, labelled by rewards assigned to each piece\n",
    "    # One-hot encoding used to separate black and white pieces\n",
    "    # Nothing: 0\n",
    "    # Pawn: 1\n",
    "    # Knight: 3\n",
    "    # Bishop: 3.15\n",
    "    # Rook: 4.5\n",
    "    # Queen: 9\n",
    "    # King: 1000\n",
    "    REWARDS = {\"P\": 1, # pawn\n",
    "               \"N\": 3, # knight\n",
    "               \"B\": 3.15, # bishop\n",
    "               \"R\": 4.5, # rook\n",
    "               \"Q\": 9, # queen\n",
    "               \"K\": 1000} # king\n",
    "    fields[0] = fields[0].split(\"/\")[::-1]\n",
    "    labels = chess_positions()\n",
    "    pieces = [[0, 0] for _ in range(64)] # list of list[white, black]\n",
    "    column = 0\n",
    "    for row in range(len(fields[0])):\n",
    "        for piece in fields[0][row]:\n",
    "            if piece.isdigit(): # nothing\n",
    "                column += int(piece)\n",
    "            else: # piece\n",
    "                if piece.isupper(): # white\n",
    "                    pieces[column][0] = REWARDS[piece.upper()]\n",
    "                else: # black\n",
    "                    pieces[column][1] = REWARDS[piece.upper()]\n",
    "                column += 1\n",
    "    piece_placements = dict(zip(labels, pieces))\n",
    "\n",
    "    # ACTIVE COLOUR\n",
    "    active_colour = [1] if fields[1] == \"w\" else [0]\n",
    "\n",
    "    # CASTLING AVAILABILITY\n",
    "    castling = [1 if \"K\" in fields[2] else 0,\n",
    "                1 if \"Q\" in fields[2] else 0,\n",
    "                1 if \"k\" in fields[2] else 0,\n",
    "                1 if \"q\" in fields[2] else 0] # [kingside_white, queenside_white, kingside_black, queenside_black]\n",
    "\n",
    "    # EN PASSANT AVAILABILITY\n",
    "    # Categorical encoding, by alphabetical label of square (e.g. if square is d6, en_passant = 4)\n",
    "    # En passant is always on 6th rank\n",
    "    # No en passant, en_passant = 0\n",
    "    en_passant = [ord(fields[3][0]) - ord(\"a\") + 1] if fields[3] != \"-\" else [0]\n",
    "    \n",
    "    ##return [[piece for square in piece_placements.values() for piece in square], active_colour, castling, en_passant]\n",
    "    return [piece for square in piece_placements.values() for piece in square] + active_colour + castling + en_passant\n",
    "\n",
    "def softmax(lst):\n",
    "    exp_list = [exp(item) for item in lst]\n",
    "    return [item / sum(exp_list) for item in exp_list]\n",
    "\n",
    "def calculate_score(board, white_turn = True):\n",
    "    score = round(sum(board_to_state(board)[:-6][::2]) - sum(board_to_state(board)[:-6][1::2]), 2)\n",
    "    if board.is_checkmate():\n",
    "        if white_turn:\n",
    "            score += 1000\n",
    "        else:\n",
    "            score -= 1000\n",
    "    return score if white_turn == True else -score\n",
    "\n",
    "def softmax_opponent_reward(board, white_turn = True, done = False):\n",
    "    # Calculate softmax reward for black if white_turn = True, and vice-versa\n",
    "    if done == True:\n",
    "        return 0.0\n",
    "    rewards = []\n",
    "    next_moves = valid_moves(board)\n",
    "    curr_score = calculate_score(board, white_turn ^ True)\n",
    "    for move in next_moves:\n",
    "        board.push(chess.Move.from_uci(move))\n",
    "        rewards.append(calculate_score(board, white_turn ^ True) - curr_score)\n",
    "        board.pop()\n",
    "    if max(rewards) > 100: # exp(1000) leads to overflow\n",
    "        return max(rewards)\n",
    "    softmax_rewards = softmax(rewards)\n",
    "    return sum([rewards[index] * softmax_rewards[index] for index in range(len(rewards))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating custom gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessEnv(Env):\n",
    "    def __init__(self, white_player = \"Magnum Carlos\", black_player = \"Magnesium Cars\", episode = 0):\n",
    "        # WORK TO DO: implement param \"board\" and \"white_turn\" for __init__() to submit game that is still ongoing\n",
    "        self.board = chess.Board()\n",
    "        self.game = chess.pgn.Game()\n",
    "        self.game.headers[\"Event\"] = \"Solving Chess Using Deep Q-Learning\"\n",
    "        self.game.headers[\"Site\"] = \"Python\"\n",
    "        self.game.headers[\"Date\"] = datetime.now().date()\n",
    "        self.game.headers[\"Round\"] = episode\n",
    "        self.game.headers[\"White\"] = white_player\n",
    "        self.game.headers[\"Black\"] = black_player\n",
    "        self.move = self.game\n",
    "        self.actions = possible_moves()\n",
    "        self.action_space = Discrete(len(self.actions) + 1) # stalemate/checkmate\n",
    "        # 7 possibilities for each of the 64 squares (x2 for white and black)\n",
    "        # Binary for active colour and castling availability\n",
    "        # 9 possibilities for en passant capture on 6th rank\n",
    "        # Multidiscrete/Box/Tuple space does not work for keras-rl2: https://stackoverflow.com/questions/70861260/training-dqn-agent-with-multidiscrete-action-space-in-gym\n",
    "        ##self.observation_space = Tuple([Discrete(7) for _ in range(128)] + [Discrete(2) for _ in range(5)] + [Discrete(9)])\n",
    "        ##self.observation_space = MultiDiscrete([7 for _ in range(128)] + [2 for _ in range(5)] + [9])\n",
    "        self.observation_space = Tuple([Discrete(9) for _ in range(len(board_to_state(self.board)))]) # there are some unused observation space, but we should compromise\n",
    "        self.white_turn = True\n",
    "        self.episode_length = 400\n",
    "        \n",
    "    def board_to_state(self): # custom board to state implemented to flatten observation space\n",
    "        PIECES = {0: 0, # nothing\n",
    "                  1: 1, # pawn\n",
    "                  3: 2, # knight\n",
    "                  3.15: 3, # bishop\n",
    "                  4.5: 5, # rook\n",
    "                  9: 6, # queen\n",
    "                  1000: 7} # king\n",
    "        state = board_to_state(self.board)\n",
    "        state = [PIECES[piece] for piece in state[:-6]] + state[-6:]\n",
    "        return [[1 if index == item else 0 for index in range(9)] for item in state]\n",
    "        \n",
    "    def step(self, action):\n",
    "        if action == len(self.actions): # stalemate/checkmate\n",
    "            self.episode_length -= 0.5\n",
    "            done = self.check_if_done()\n",
    "            self_reward = 0 - calculate_score(self.board, self.white_turn)\n",
    "            return tuple(self.board_to_state()), self_reward, done, {} # observation, reward, done, info\n",
    "        \n",
    "        action = self.actions[action]\n",
    "        curr_score = calculate_score(self.board, self.white_turn)\n",
    "        \n",
    "        self.board.push(chess.Move.from_uci(action)) # next state\n",
    "        self.move = self.move.add_variation(chess.Move.from_uci(action))\n",
    "            \n",
    "        self_reward = calculate_score(self.board, self.white_turn) - curr_score\n",
    "        \n",
    "        self.episode_length -= 0.5 # half move\n",
    "        done = self.check_if_done()\n",
    "        \n",
    "        self_reward -= softmax_opponent_reward(self.board, self.white_turn, done) # take opponent move into consideration\n",
    "        \n",
    "        self.white_turn ^= True # pass back turn to opponent\n",
    "        \n",
    "        return tuple(self.board_to_state()), self_reward, done, {} # observation, reward, done, info\n",
    "    \n",
    "    def render(self, mode = \"human\"):\n",
    "        return chess.svg.board(self.board, size = 350)  \n",
    "    \n",
    "    def reset(self ,get_last_game = False):\n",
    "        white_player, black_player, episode = self.game.headers[\"White\"], self.game.headers[\"Black\"], self.game.headers[\"Round\"]\n",
    "        \n",
    "        if episode % 5 == 0 and episode > 0:\n",
    "            if self.game.headers[\"Result\"] != \"*\":\n",
    "                print(self.game, file = open(f\"games/game_{episode}.pgn\", \"w\"), end = \"\\n\\n\")\n",
    "        \n",
    "        del self.game\n",
    "        del self.board\n",
    "        del self.action_space\n",
    "        del self.observation_space\n",
    "        gc.collect()\n",
    "        if get_last_game:\n",
    "            episode -= 1\n",
    "        self.__init__(white_player, black_player, episode + 1)\n",
    "        return tuple(self.board_to_state())\n",
    "    \n",
    "    def check_if_done(self):\n",
    "        if self.board.is_checkmate():\n",
    "            self.game.headers[\"Result\"] = \"1-0\" if self.white_turn is True else \"0-1\"\n",
    "            return True\n",
    "        elif self.board.can_claim_draw() or self.board.is_insufficient_material() or self.board.is_stalemate() or self.episode_length <= 0:\n",
    "            self.game.headers[\"Result\"] = \"1/2-1/2\"\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "class CustomChessEnv(ChessEnv):\n",
    "    def __init__(self, white_player = \"Magnum Carlos\", black_player = \"Magnesium Cars\", episode = 0):\n",
    "        super().__init__(white_player, black_player, episode)\n",
    "        if episode % 5 == 1: # common openings\n",
    "            action = \"e2e4\"\n",
    "        elif episode % 5 == 2:\n",
    "            action = \"d2d4\"\n",
    "        elif episode % 5 == 3:\n",
    "            action = \"c2c4\"\n",
    "        else:\n",
    "            allowed_actions = list(valid_moves(self.board))\n",
    "            allowed_actions.remove(\"e2e4\")\n",
    "            allowed_actions.remove(\"d2d4\")\n",
    "            allowed_actions.remove(\"c2c4\")\n",
    "            action = choice(allowed_actions)\n",
    "        \n",
    "        self.board.push(chess.Move.from_uci(action))\n",
    "        self.move = self.move.add_variation(chess.Move.from_uci(action))\n",
    "    \n",
    "    def reset(self, get_last_game = False):\n",
    "        white_player, black_player, episode = self.game.headers[\"White\"], self.game.headers[\"Black\"], self.game.headers[\"Round\"]\n",
    "        \n",
    "        if self.game.headers[\"Result\"] != \"*\":\n",
    "            print(self.game, file = open(f\"games/game_{episode}.pgn\", \"w\"), end = \"\\n\\n\")\n",
    "        \n",
    "        del self.game\n",
    "        del self.board\n",
    "        del self.action_space\n",
    "        del self.observation_space\n",
    "        gc.collect()\n",
    "        \n",
    "        if get_last_game:\n",
    "            episode -= 1\n",
    "        self.__init__(white_player, black_player, episode + 1)\n",
    "        return tuple(self.board_to_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChessEnv()\n",
    "observation_space_size = (1, len(env.observation_space), 9)\n",
    "action_space_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating deep RL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1, 134, 9)]       0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1, 134, 32)        320       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1, 134, 16)        528       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1, 134, 8)         136       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1072)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1969)              2112737   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,113,721\n",
      "Trainable params: 2,113,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(observation_space_size, action_space_size, hidden_layers = 3, nodes = 32):\n",
    "    inputs = Input(shape = observation_space_size)\n",
    "    hidden = []\n",
    "    for _ in range(hidden_layers):\n",
    "        if len(hidden) == 0:\n",
    "            hidden.append(Dense(nodes, activation = \"relu\", kernel_initializer = \"he_normal\")(inputs))\n",
    "        else:\n",
    "            hidden.append(Dense(nodes, activation = \"relu\", kernel_initializer = \"he_normal\")(hidden[-1]))\n",
    "        nodes //= 2 # Decreasing number of nodes increases efficiency\n",
    "    hidden.append(Flatten()(hidden[-1]))\n",
    "    outputs = Dense(action_space_size, activation = \"linear\")(hidden[-1])\n",
    "    return Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "model = create_model(observation_space_size, action_space_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "LEARNING_RATE = 0.05\n",
    "EXPLORATION_RATE = 0.9999\n",
    "EXPLORATION_DECAY = 0.999999\n",
    "\n",
    "class EpsilonGreedyPolicy(EpsGreedyQPolicy): # custom policy\n",
    "    def __init__(self, eps, env):\n",
    "        super().__init__(eps = eps)\n",
    "        self.env = env\n",
    "    \n",
    "    def select_action(self, q_values):\n",
    "        assert q_values.ndim == 1\n",
    "        \n",
    "        # Invalid action masking\n",
    "        allowed_actions = valid_moves(self.env.board)\n",
    "        action_space = self.env.actions\n",
    "        allowed_q_values = {action_space[index]: q_values[index] for index in range(len(action_space)) if action_space[index] in allowed_actions}\n",
    "\n",
    "        if len(allowed_actions) == 0:\n",
    "            return len(action_space) # stalemate/checkmate\n",
    "        \n",
    "        if random() < self.eps: # explore, random action\n",
    "            action = choice(allowed_actions)\n",
    "        else: # exploit, best action\n",
    "            action = max(allowed_q_values, key = allowed_q_values.get)\n",
    "        action = action_space.index(action)\n",
    "        \n",
    "        self.eps *= EXPLORATION_DECAY\n",
    "        return action\n",
    "    \n",
    "class GreedyPolicy(GreedyQPolicy):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "    \n",
    "    def select_action(self, q_values):\n",
    "        assert q_values.ndim == 1\n",
    "        \n",
    "        # Invalid action masking\n",
    "        allowed_actions = valid_moves(self.env.board)\n",
    "        action_space = self.env.actions\n",
    "        allowed_q_values = {action_space[index]: q_values[index] for index in range(len(action_space)) if action_space[index] in allowed_actions}\n",
    "        \n",
    "        if len(allowed_actions) == 0:\n",
    "            return len(action_space) # stalemate/checkmate\n",
    "        \n",
    "        action = max(allowed_q_values, key = allowed_q_values.get)\n",
    "        action = action_space.index(action)\n",
    "        return action\n",
    "\n",
    "def create_agent(model, action_space_size, env):\n",
    "    policy = EpsilonGreedyPolicy(EXPLORATION_RATE, env)\n",
    "    test_policy = GreedyPolicy(env)\n",
    "    memory = SequentialMemory(limit = 100000, window_length = 1) # may want to separate memory into black and white, early and mid and endgame\n",
    "    # Regarding target_model_update: https://github.com/keras-rl/keras-rl/issues/55\n",
    "    dqn = DQNAgent(model = model, memory = memory, policy = policy, test_policy = test_policy, nb_actions = action_space_size, nb_steps_warmup = 100, target_model_update = 1e-2, gamma = DISCOUNT_FACTOR)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 400000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 952s 95ms/step - reward: -12.2477\n",
      "28 episodes - episode_reward: -4285.523 [-10795.561, -259.878] - loss: 157979543131341824.000 - mean_q: 3303853289.479\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 949s 95ms/step - reward: -14.17330s - reward: -\n",
      "29 episodes - episode_reward: -4957.612 [-16589.937, -393.690] - loss: 190134992169722707968.000 - mean_q: 236332646400.000\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 995s 99ms/step - reward: -15.1703\n",
      "28 episodes - episode_reward: -5377.110 [-17437.958, -361.314]\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 1092s 109ms/step - reward: -15.8222\n",
      "29 episodes - episode_reward: -5310.128 [-17845.523, -356.388]\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 1003s 100ms/step - reward: -17.9797 - reward: -1\n",
      "30 episodes - episode_reward: -6120.297 [-29434.273, -296.457]\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 1006s 101ms/step - reward: -15.0232\n",
      "34 episodes - episode_reward: -4461.119 [-21050.364, -345.225]\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 1044s 104ms/step - reward: -14.1033\n",
      "30 episodes - episode_reward: -4714.768 [-19728.640, -250.528]\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 568s 57ms/step - reward: -12.7804\n",
      "32 episodes - episode_reward: -3595.117 [-23442.966, -67.488]\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 631s 63ms/step - reward: -17.5287\n",
      "31 episodes - episode_reward: -5992.754 [-14566.659, -3.489]\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 549s 55ms/step - reward: -12.4272\n",
      "28 episodes - episode_reward: -4503.214 [-18850.585, -185.219]\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 546s 55ms/step - reward: -13.1086\n",
      "30 episodes - episode_reward: -4302.179 [-13847.299, -529.796]\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 569s 57ms/step - reward: -12.9385\n",
      "28 episodes - episode_reward: -4710.603 [-10685.949, -496.068]\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 577s 58ms/step - reward: -15.7422\n",
      "30 episodes - episode_reward: -5250.371 [-16524.112, -115.631]\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 1045s 105ms/step - reward: -19.7035\n",
      "28 episodes - episode_reward: -6908.037 [-18659.697, -437.339]\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 1271s 127ms/step - reward: -16.1491\n",
      "30 episodes - episode_reward: -5323.574 [-19679.930, -593.648]\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 1172s 117ms/step - reward: -14.2650\n",
      "27 episodes - episode_reward: -5447.706 [-23416.238, -182.412]\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 1187s 119ms/step - reward: -19.7229\n",
      "32 episodes - episode_reward: -5890.742 [-26165.597, -207.357]\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 1165s 117ms/step - reward: -10.8905\n",
      "30 episodes - episode_reward: -3951.984 [-12835.196, -486.350]\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 1198s 120ms/step - reward: -12.6817\n",
      "30 episodes - episode_reward: -4151.955 [-14407.781, -292.899]\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 1189s 119ms/step - reward: -15.8417\n",
      "31 episodes - episode_reward: -5182.821 [-18512.627, -244.587]\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 579s 58ms/step - reward: -23.5540\n",
      "29 episodes - episode_reward: -8123.939 [-55472.267, -425.070]\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 550s 55ms/step - reward: -12.6286\n",
      "26 episodes - episode_reward: -4754.031 [-17321.292, -324.373]\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 565s 57ms/step - reward: -16.0636\n",
      "30 episodes - episode_reward: -5340.729 [-21734.501, -253.814]\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 564s 56ms/step - reward: -14.3544\n",
      "29 episodes - episode_reward: -4977.901 [-15454.767, -135.683]\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 591s 59ms/step - reward: -12.3221\n",
      "27 episodes - episode_reward: -4606.283 [-27763.784, -456.553]\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 568s 57ms/step - reward: -14.0018\n",
      "30 episodes - episode_reward: -4688.423 [-15830.015, -4.322]\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 571s 57ms/step - reward: -22.8584\n",
      "30 episodes - episode_reward: -7553.677 [-26179.893, -407.224]\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 585s 59ms/step - reward: -19.5444\n",
      "31 episodes - episode_reward: -6349.499 [-26427.918, -16.235]\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 592s 59ms/step - reward: -14.8246\n",
      "33 episodes - episode_reward: -4522.006 [-18861.790, -68.147]\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 574s 57ms/step - reward: -12.5234\n",
      "29 episodes - episode_reward: -4317.426 [-14865.777, -350.923]\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 575s 58ms/step - reward: -12.0065\n",
      "30 episodes - episode_reward: -3937.696 [-13318.748, -269.416]\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 569s 57ms/step - reward: -14.6621\n",
      "27 episodes - episode_reward: -5444.041 [-20729.060, -341.254]\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 588s 59ms/step - reward: -15.7244\n",
      "32 episodes - episode_reward: -4724.568 [-13653.341, -18.415]\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 590s 59ms/step - reward: -19.7433\n",
      "31 episodes - episode_reward: -6614.704 [-28854.275, -48.114]\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 600s 60ms/step - reward: -15.3673\n",
      "33 episodes - episode_reward: -4660.539 [-16634.848, -126.213]\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 1150s 115ms/step - reward: -20.7059\n",
      "30 episodes - episode_reward: -6872.547 [-22887.692, -29.522]\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 1198s 120ms/step - reward: -7.9593\n",
      "36 episodes - episode_reward: -2198.178 [-13400.524, -3.797]\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 1188s 119ms/step - reward: -14.4726\n",
      "32 episodes - episode_reward: -4557.981 [-21078.568, -65.724]\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 1232s 123ms/step - reward: -19.5394\n",
      "34 episodes - episode_reward: -5681.870 [-16126.551, -306.915]\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 1245s 124ms/step - reward: -19.0548\n",
      "done, took 33383.494 seconds\n"
     ]
    }
   ],
   "source": [
    "dqn = create_agent(model, action_space_size, env)\n",
    "optimizer = Adam(learning_rate = LEARNING_RATE)\n",
    "dqn.compile(optimizer)\n",
    "dqn.fit(env, nb_steps = 400000, visualize = False, verbose = 1)\n",
    "_ = env.reset(get_last_game = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -22.064, steps: 26\n",
      "Episode 2: reward: -22.064, steps: 26\n",
      "Episode 3: reward: -22.064, steps: 26\n",
      "Episode 4: reward: -22.064, steps: 26\n",
      "Episode 5: reward: -22.064, steps: 26\n"
     ]
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes = 5, visualize = False)\n",
    "_ = env.reset(get_last_game = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Saving and loading the DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_weights.h5f', overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomChessEnv(env.game.headers[\"White\"], env.game.headers[\"Black\"], env.game.headers[\"Round\"])\n",
    "observation_space_size = (1, len(env.observation_space), 9)\n",
    "action_space_size = env.action_space.n\n",
    "model = create_model(observation_space_size, action_space_size)\n",
    "dqn = create_agent(model, action_space_size, env)\n",
    "optimizer = Adam(learning_rate = LEARNING_RATE)\n",
    "dqn.compile(optimizer)\n",
    "dqn.load_weights('dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -5.277, steps: 29\n",
      "Episode 2: reward: -0.753, steps: 28\n",
      "Episode 3: reward: -2.949, steps: 23\n",
      "Episode 4: reward: -0.803, steps: 28\n",
      "Episode 5: reward: 0.000, steps: 15\n"
     ]
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes = 5, visualize = True)\n",
    "_ = env.reset(get_last_game = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results\n",
    "Though I did not tune the various hyperparameters, what was certain was that deep q-learning was especially bad at solving chess, in which the main reason lies in stability. Without a stable opponent, it is unable to determine whether a move was good or bad. Furthermore, with the analysis of a particular game my model has played (https://www.chess.com/analysis/game/pgn/h1TkS1t1U), we can see that the agent is unable to take advantage of the opponent's blunders and kept on \"sacrificing\" pieces for no material or positional advantage. Moreover, it is unable to play endgames well, though it might if we were to create 3 separate agents to play openings, middlegame and endgame separately. The agent is also performing quite badly as seen from the agent drawing winnable games through repetition or going through 50 moves without a capture.\n",
    "\n",
    "Based on the large loss and mean_q values produced by the model, it could be due to the exploding gradients problem. We can solve this using through clipping (passing a clipvalue/clipnorm argument to our optimizer), but I believe this problem is second to instability. The problem of stability can still be solved if the model was playing against a stable agent like Stockfish.\n",
    "\n",
    "Though, if there is anything that my agent learnt how to do, it was to make quick draws like super GMs do. Also it knows how to develop the knights first."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
